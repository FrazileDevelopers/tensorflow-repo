{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Customizing_Week4_Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8V-dWfuU7MLO",
        "VETT8KFC7MLY",
        "omOPHxfJ7MLi",
        "4AXpmrZ-7MLo",
        "H29iRg5l7MLx",
        "HTleeosW7ML4",
        "KTEaudvN7ML8",
        "-ryLItfi7MMB",
        "myUlJq2_7MME",
        "tuk_pQ7A7MMI",
        "1V7P3raP7MMN",
        "FWtoHyZK7MMT",
        "xzeCqC697MMV",
        "IQWvJS8y7MMZ",
        "laUe5LLl7MMe",
        "u8IOTSRz7MMn",
        "01XZhNmj7MMt"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et4CIcCA7MJs",
        "colab_type": "code",
        "outputId": "a8f4b253-e687-4658-9977-93372ef82365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JF8WqYZtjGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl7vSLrj7MJz",
        "colab_type": "text"
      },
      "source": [
        "# Model subclassing and custom training loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80YEgRgw7MJ1",
        "colab_type": "text"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Model subclassing](#coding_tutorial_1)\n",
        " #### [2. Custom layers](#coding_tutorial_2)\n",
        " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
        " #### [4. Custom training loops](#coding_tutorial_4)\n",
        " #### [5. tf.function decorator](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmfP80h27MJ4",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a27-eSW7MJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YWodM17MKA",
        "colab_type": "text"
      },
      "source": [
        "#### Create a simple model using the model subclassing API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9GYLA_j7MKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "\n",
        "class MyModel(Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense_1 = Dense(64, activation='relu')\n",
        "    self.dense_2 = Dense(10)\n",
        "    self.dropout = Dropout(0.4)\n",
        "\n",
        "  def call(self, inputs, training=True):\n",
        "    x = self.dense_1(inputs)\n",
        "    if training:\n",
        "      x = self.dropout(x)\n",
        "    return self.dense_2(x)\n",
        "\n",
        "# define a model subclass denoted as MyModel\n",
        "# The objects that you will generate from this class are models with two dense layers. Then within the init method, \n",
        "# you can define the two dense layers.\n",
        "# in the call method, you can set up the forward pass by calling the previously defined dense layers.\n",
        "# Firstly you can place the Dense 1 layer to take the inputs\n",
        "# after it, the Dense 2 layer that returns the outputs of the layer.\n",
        "\n",
        "# Now go back and modify the call method by including a keyword argument called training.\n",
        "# This is useful if you want to have different behavior in training and in inference.\n",
        "# For example, for this, you can create a dropout layer that will be activated only if training is true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvDSeFcA7MKG",
        "colab_type": "code",
        "outputId": "ad92e2ac-af23-4d9c-b4e9-a1a096161c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model = MyModel()\n",
        "model(tf.random.uniform([1,10]))\n",
        "model.summary()\n",
        "\n",
        "# The next step is to instantiate the model. \n",
        "# For this, just call the class without any argument inside the brackets.\n",
        "# Next, call the model on a random input to create the weights.\n",
        "# For the input, I guess using a one-dimensional vector with 10 elements.\n",
        "# After, you can print out model.summary, where you can see the dense layers that you defined before."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  704       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  650       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 1,354\n",
            "Trainable params: 1,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuZ-2O4C-Ble",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "\n",
        "class MyModel(Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense_1 = Dense(64, activation='relu')\n",
        "    self.dense_2 = Dense(10)\n",
        "    self.dense_3 = Dense(5)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    y1 = self.dense_2(inputs)\n",
        "    y2 = self.dense_3(y1)\n",
        "    concat = concatenate([x, y2])\n",
        "    return self.softmax(concat)\n",
        "\n",
        "# you can experiment with concatenating the outputs of two layers.\n",
        "# This is useful if you want to define us obtain non-linear topology in the model.\n",
        "# add a third dense layer of five units together with a softmax layer to the init method.\n",
        "# You can create two branches by playing with the layers in the network.\n",
        "# One branch has only the dense one layer, the other has the dense two, a dense three layers sequentially.\n",
        "# Then the outputs of both branches can be concatenated by just writing concatenate.\n",
        "# At the end, you can set up a softmax layer for the output of the concatenation."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-8wwjfS-no4",
        "colab_type": "code",
        "outputId": "9749a09c-d64d-4f2b-e857-53cb9d02ab12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model = MyModel()\n",
        "model(tf.random.uniform([1,10]))\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              multiple                  704       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  110       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  55        \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 869\n",
            "Trainable params: 869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACFu79n7MKM",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDL5j8na7MKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHffugUp7MKV",
        "colab_type": "text"
      },
      "source": [
        "#### Create custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocmAGCmB7MKX",
        "colab_type": "code",
        "outputId": "36368528-6a00-45aa-f1bf-89f25fcd6c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# Create a custom layer\n",
        "\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape = (input_dim, units),\n",
        "                             initializer = 'random_normal')\n",
        "    self.b = self.add_weight(shape = (units,),\n",
        "                             initializer = 'zeros')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "dense_layer = MyLayer(3, 5)\n",
        "x = tf.ones((1, 5))\n",
        "print(dense_layer(x))\n",
        "print(dense_layer.weights)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[1. 1. 1. 1. 1.]], shape=(1, 5), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
            "array([[ 0.04238772, -0.01499842,  0.03144122],\n",
            "       [-0.00376119,  0.01006788,  0.00616314],\n",
            "       [-0.00847362,  0.03943233, -0.02595833],\n",
            "       [ 0.02076507, -0.05136209,  0.00939988],\n",
            "       [-0.02356167,  0.01731388,  0.08058437]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_yfOoZo7MKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify trainable weights\n",
        "\n",
        "class MyLayer(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayer, self).__init__()\n",
        "    self.w = self.add_weight(shape = (input_dim, units),\n",
        "                             initializer = 'random_normal',\n",
        "                             trainable=False)\n",
        "    self.b = self.add_weight(shape = (units,),\n",
        "                             initializer = 'zeros',\n",
        "                             trainable=False)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "dense_layer = MyLayer(3, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT3kUMzK7MKk",
        "colab_type": "code",
        "outputId": "5a0a9b9f-4f54-4145-d452-f0d2efe049bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('trainable weights:', len(dense_layer.trainable_weights))\n",
        "print('non-trainable weights:', len(dense_layer.non_trainable_weights))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainable weights: 0\n",
            "non-trainable weights: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8NU14t17MKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a custom layer to accumulate means of output values\n",
        "\n",
        "class MyLayerMean(Layer):\n",
        "\n",
        "  def __init__(self, units, input_dim):\n",
        "    super(MyLayerMean, self).__init__()\n",
        "    self.w = self.add_weight(shape = (input_dim, units),\n",
        "                             initializer = 'random_normal')\n",
        "    self.b = self.add_weight(shape = (units,),\n",
        "                             initializer = 'zeros')\n",
        "    self.sum_activation = tf.Variable(initial_value=tf.zeros((units,)),\n",
        "                                      trainable=False)\n",
        "    self.number_call = tf.Variable(initial_value=0,\n",
        "                                   trainable=False)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "      activations = tf.matmul(inputs, self.w) + self.b\n",
        "      self.sum_activation.assign_add(tf.reduce_sum(activations, axis=0))\n",
        "      self.number_call.assign_add(inputs.shape[0])\n",
        "      return activations, self.sum_activation / tf.cast(self.number_call, tf.float32)\n",
        "\n",
        "dense_layer = MyLayerMean(3, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbGvIpNv7MKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the layer\n",
        "\n",
        "y, activation_means = dense_layer(tf.ones((1, 5)))\n",
        "print(activation_means.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CeyAa6l7MK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Dropout layer as a custom layer\n",
        "\n",
        "class MyDropout(Layer):\n",
        "\n",
        "    def __init__(self, rate):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.rate = rate\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass for dropout layer\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zjah9zC7MK9",
        "colab_type": "text"
      },
      "source": [
        "#### Implement the custom layers into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72gKLG7k7MK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model using custom layers with the model subclassing API\n",
        "\n",
        "class MyModel(Model):\n",
        "\n",
        "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer_1 = MyLayer(units_1, input_dim_1)\n",
        "        self.dropout_1 = MyDropout(0.5)\n",
        "        self.layer_2 = MyLayer(units_2, units_1)\n",
        "        self.dropout_2 = MyDropout(0.5)\n",
        "        self.layer_3 = MyLayer(units_3, units_2)\n",
        "        self.softmax = Softmax()\n",
        "           \n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x = self.layer_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.layer_3(x)\n",
        "\n",
        "        return self.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgbyBXXg7MLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "29f8e6e3-001e-4af7-bbd8-9e24f2426cb9"
      },
      "source": [
        "# Instantiate a model object\n",
        "\n",
        "model = MyModel(64,10000,64,46)\n",
        "print(model(tf.ones((1, 10000))))\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[3.8417117e-04 7.0363408e-06 7.0363408e-06 ... 7.0363408e-06\n",
            "  7.0363408e-06 7.0363408e-06]], shape=(1, 10000), dtype=float32)\n",
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "my_layer_2 (MyLayer)         multiple                  640064    \n",
            "_________________________________________________________________\n",
            "my_dropout (MyDropout)       multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_3 (MyLayer)         multiple                  4160      \n",
            "_________________________________________________________________\n",
            "my_dropout_1 (MyDropout)     multiple                  0         \n",
            "_________________________________________________________________\n",
            "my_layer_4 (MyLayer)         multiple                  2990      \n",
            "_________________________________________________________________\n",
            "softmax_1 (Softmax)          multiple                  0         \n",
            "=================================================================\n",
            "Total params: 647,214\n",
            "Trainable params: 0\n",
            "Non-trainable params: 647,214\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLMpvE927MLK",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9U1m1am7MLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V-dWfuU7MLO",
        "colab_type": "text"
      },
      "source": [
        "#### Create synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-HpHvyu7MLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b0b73221-e03d-43fc-fb5e-aa24ae96555f"
      },
      "source": [
        "# Create data from a noise contaminated linear model\n",
        "\n",
        "def MakeNoisyData(m, b, n=20):\n",
        "    x = tf.random.uniform(shape=(n,))\n",
        "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
        "    y = m * x + b + noise\n",
        "    return x, y\n",
        "\n",
        "m=1\n",
        "b=2\n",
        "x_train, y_train = MakeNoisyData(m,b)\n",
        "plt.plot(x_train, y_train, 'b.')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa7bd3f2d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VETT8KFC7MLY",
        "colab_type": "text"
      },
      "source": [
        "#### Define a linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXlPYKTF7MLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVbYB6nd7MLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "b2f38e45-a6e9-4c84-f37e-09fe71967941"
      },
      "source": [
        "# Build a custom layer for the linear regression model\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(LinearLayer, self).__init__()\n",
        "    self.m = self.add_weight(shape=(1,),\n",
        "                             initializer = 'random_normal')\n",
        "    self.b = self.add_weight(shape=(1,),\n",
        "                             initializer = 'zeros')\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return self.m*inputs+self.b\n",
        "\n",
        "linear_regression = LinearLayer()\n",
        "\n",
        "print(linear_regression(x_train))\n",
        "print(linear_regression.weights)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.0589023  0.05523419 0.01786437 0.04614269 0.06023839 0.02309819\n",
            " 0.04452568 0.02998605 0.03347091 0.01241871 0.04737506 0.05283008\n",
            " 0.01349668 0.03857351 0.01064908 0.03635269 0.00274625 0.012217\n",
            " 0.03741079 0.06039034], shape=(20,), dtype=float32)\n",
            "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.06122905], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omOPHxfJ7MLi",
        "colab_type": "text"
      },
      "source": [
        "#### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHrAuQY87MLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0851f31-84ad-49d6-bb50-83e9f30de23d"
      },
      "source": [
        "# Define the mean squared error loss function\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
        "\n",
        "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
        "print(\"Starting loss\", starting_loss.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting loss 6.530937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AXpmrZ-7MLo",
        "colab_type": "text"
      },
      "source": [
        "#### Train and plot the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jikva3To7MLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "fd6d43be-b2f3-4daf-82d7-e777c9304278"
      },
      "source": [
        "# Implement a gradient descent training loop for the linear regression model\n",
        "\n",
        "learning_rate = 0.05\n",
        "steps = 25\n",
        "\n",
        "for i in range(steps):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = linear_regression(x_train)\n",
        "    loss = SquaredError(predictions, y_train)\n",
        "  gradients = tape.gradient(loss, linear_regression.trainable_variables)\n",
        "\n",
        "  linear_regression.m.assign_sub(learning_rate*gradients[0])\n",
        "  linear_regression.b.assign_sub(learning_rate*gradients[1])\n",
        "\n",
        "  print('step %d, loss %f' % (i, loss.numpy()))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, loss 6.530937\n",
            "step 1, loss 4.895776\n",
            "step 2, loss 3.670455\n",
            "step 3, loss 2.752250\n",
            "step 4, loss 2.064185\n",
            "step 5, loss 1.548577\n",
            "step 6, loss 1.162202\n",
            "step 7, loss 0.872668\n",
            "step 8, loss 0.655702\n",
            "step 9, loss 0.493117\n",
            "step 10, loss 0.371282\n",
            "step 11, loss 0.279983\n",
            "step 12, loss 0.211567\n",
            "step 13, loss 0.160298\n",
            "step 14, loss 0.121879\n",
            "step 15, loss 0.093089\n",
            "step 16, loss 0.071514\n",
            "step 17, loss 0.055346\n",
            "step 18, loss 0.043230\n",
            "step 19, loss 0.034151\n",
            "step 20, loss 0.027346\n",
            "step 21, loss 0.022247\n",
            "step 22, loss 0.018425\n",
            "step 23, loss 0.015561\n",
            "step 24, loss 0.013414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLb87KvN7MLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "3e6d0483-cc60-4ab7-9ee2-3f9282424b66"
      },
      "source": [
        "# Plot the learned regression model\n",
        "\n",
        "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
        "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
        "\n",
        "plt.plot(x_train, y_train, 'b.')\n",
        "\n",
        "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
        "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m:1,  trained m:[1.1651877]\n",
            "b:2,  trained b:[1.8377671]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa7d95a1438>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAVLklEQVR4nO3db5BkV3nf8e9P/wwuyYiS1i4itKxdQGwKGYmMDVtyxWuEHUElqFIhLgwI2yVny47jksp6QSKqXDZ64ZA/CnGBLa+tAHKUGIwUe0s2ThRZGwJZLZkVQivt2lgGRwi2wiBAwlBBXunJi+41o9H0dPfMvbe7b38/VVPb3fdM97l3Z545/fRzzklVIUlafGfMugOSpGYY0CWpJwzoktQTBnRJ6gkDuiT1hAFdknpibEBP8pwkn0jyqSQPJfmVTdr8YpLjSR5IcneSF7XTXUnSKJOM0L8JvKaqXgFcClyZ5NUb2nwSWKmq7wc+DPyrZrspSRrnrHENajDz6K+Gd88eftWGNvesu3sv8NZxz3vhhRfWnj17Ju6oJAmOHj36paratdmxsQEdIMmZwFHgxcB7q+rIFs2vAT4y7jn37NnD6urqJC8vSRpK8n9GHZvoQ9GqeqqqLgVeCPxgkpePeKG3AivAvx5xfH+S1SSra2trk7y0JGlCU1W5VNVXgXuAKzceS/Ja4B3AG6rqmyO+/0BVrVTVyq5dm75jkCRt0yRVLruSnD+8/VzgR4E/3dDmMuA3GQTzL7bRUUnS1ibJob8A+MAwj34G8KGqujPJO4HVqjrIIMVyLvB7SQAeqao3tNVpSdKzTVLl8gBw2SaP/9K6269tuF+SpCk5U1SSesKALkkdOnwYfvVXB/82baI6dEnSzh0+DFdcAU8+CeecA3ffDXv3Nvf8jtAlqSOHDg2C+VNPDf49dKjZ5zegS1JH9u0bjMzPPHPw7759zT6/KRdJ6sjevYM0y6FDg2DeZLoFDOiS1Km9e5sP5KeZcpGknjCgS1JPGNAlqScM6JLUEwZ0SepIm7NEwSoXSepE27NEwRG6JHWi7VmiYECXpE60PUsUTLlIUifaniUKBnRJ6kybs0TBlIsk9YYBXZJ6woAuST1hQJeknjCgS1p6bc/g7IpVLpKWWhczOLsydoSe5DlJPpHkU0keSvIrm7T5tiQfTPJwkiNJ9rTRWUlqWhczOLsyScrlm8BrquoVwKXAlUlevaHNNcBXqurFwL8D3tVsNyWpHV3M4OzK2JRLVRXwV8O7Zw+/akOzq4BfHt7+MPCeJBl+ryTNrS5mcHZlohx6kjOBo8CLgfdW1ZENTS4CPgdQVaeSPA5cAHxpw/PsB/YD7N69e2c9l6SGtD2D8xkOH27tr8dEAb2qngIuTXI+8F+SvLyqHpz2xarqAHAAYGVlxdG7pN7ZMl63/AnsVFUuVfXVJPcAVwLrA/rngYuBR5OcBTwPeKyxXkrSAhgbrzf7BLbBgD5Jlcuu4cicJM8FfhT40w3NDgI/Obz9RuBPzJ9LWjZjK2Za/gR2khH6C4APDPPoZwAfqqo7k7wTWK2qg8AtwO8keRj4MvCmRnspSQvgdLx+8kn4oTMP8+ZHDsHhfd8ahbf8CWxmNZBeWVmp1dXVmby2JLXl8GH481sP85b3XcGZp5rPlSc5WlUrmx1z6r8kNWjvXnjb7kODYN7xbCUDuiRt16hFYGY0W8m1XCRpO7YqaZnRbCUDuiRtx7gSxE5nKw2YcpGk7ZjDRWAcoUvSOJtN/5zDRWAM6JK0lXG58jkI5KeZcpGkrSzQgukGdEk6bbMyxDnMlY9iykWSYHRqZQ5z5aMY0CUJti5DnLNc+SimXCQJFiq1MoojdEnLZ0HKEKdlQJfUK2N3eFugMsRpGdAl9cZEO7y1vGvQLJlDl9QbG2P1n9+62GWI03KELqk3Nu4Y9Jb3XQEbN5noQa58FAO6pNaMzWc3bH2sfvMjhzjztxa7DHFaBnRJrZgon92Cv4nVh/fBB875Vgd6lFoZxRy6pFZ0tgTKqF2DTg/Xb7yxu78mM+YIXVIr1uezWxsgj3sb0NPUyigGdEmt6OSzxx6XIG7H2ICe5GLgVuC7gAIOVNW/39DmecB/BHYPn/PfVNX7mu+upEXS+gC5k7cBi2OSEfop4Pqqui/JecDRJHdV1fF1bX4eOF5V/yDJLuDPktxWVU+20WlJS6in0/WbNDagV9VJ4OTw9teSnAAuAtYH9ALOSxLgXODLDP4QSNLO9Xi6fpOmqnJJsge4DDiy4dB7gO8DvgAcA66tqqcb6J8kLdSuQbM0cUBPci5wO3BdVT2x4fDfA+4H/hZwKfCeJN+xyXPsT7KaZHVtbW0H3ZbUWwu+a9AsTVTlkuRsBsH8tqq6Y5MmPw38y6oq4OEknwW+F/jE+kZVdQA4ALCyslI76bik+dHYjNAe7Bo0S5NUuQS4BThRVTeNaPYIcAXwP5N8F/C3gc801ktJc6vRGaE92DVoliYZoV8OXA0cS3L/8LEbGJQoUlU3AzcC709yDAjw9qr6Ugv9lTRnGi0FtwxxRyapcvkYgyC9VZsvAD/WVKckLY5tx2DLEBvnTFFpTnS9MmFTthWDLUNshQFdmgOzWpmwKVPHYKfst8LVFqU50Nsy61ErIVqG2ApH6NIc6OVngePSKubKG2dAl+ZAL+PbuLSKufLGGdClOdG7+NbLtx3zzYAuaecsQZwLBnRJO2MJ4tywykXSzvS2RGfxGNAlTc6VEOeaKRdJk3ElxLlnQJc0GVdCnHsGdElbOl3A8vcv2MclliHONQO6pGcbRvFjF+zjiuv28uSTcOM5ezny7ru55LFDplbmlAFd0jOty5V/7xnn8Mqn7ubjTw+C+p2P7eWSf2Egn1dWuUh6pnW58rOeepLXnHHIApYF4QhdWgCtrZW+2ROvm7Kfc87hH797H899zCzLIjCgS3OutbXSJyxDvGTvXi5p4OXUPgO6NOda2wvCMsTeMYcuzbnWJmI6w7N3HKFLc27HEzFHJeCd4dk7qaqZvPDKykqtrq7O5LWlpbHom5XqWZIcraqVzY6ZcpH6zJUQl8rYgJ7k4iT3JDme5KEk145oty/J/cM2/6P5rkrakishLr1JcuingOur6r4k5wFHk9xVVcdPN0hyPvDrwJVV9UiS72ypv5I240qIYoKAXlUngZPD219LcgK4CDi+rtmbgTuq6pFhuy+20FdJo1iCKKbMoSfZA1wGHNlw6KXA85McSnI0ydtGfP/+JKtJVtfW1rbTX0mbMbUipihbTHIucDtwXVU9scnz/B3gCuC5wOEk91bVp9c3qqoDwAEYVLnspOPS0nJDZo0wUUBPcjaDYH5bVd2xSZNHgceq6uvA15N8FHgF8OlN2kraLjdk1hYmqXIJcAtwoqpuGtHsD4AfSnJWkm8HXgWcaK6bkgDLELWlSUbolwNXA8eS3D987AZgN0BV3VxVJ5L8MfAA8DTw21X1YBsdlpbGmJUQzZVrI2eKSvNoq9RKa2vpahFsNVPUtVykeWQZorbBqf/SPLIMUdvgCF2aNcsQ1RADujRLliGqQaZcpFmyDFENMqBL27TZ4oZTNzZXrgaZcpG2Yap9I8alVcyVqyEGdGkbptq4eVxjc+VqiCkXaRumypSYVlFHHKFL2zAyU2IJombIqf9SU9yQWR1wk2ipC5YgasYM6NJ27HBD5qlKHqUJmUOXprXDDZnNzKgtBnRpWjtcCXGqkkdpCqZcpGntsAzRKka1xRG6tJUWyhCtYlRbLFuURjHZrTlk2aK0HZYhasEY0CVXQlRPmEPXcnMlRPWIAV3LzZUQ1SNjUy5JLk5yT5LjSR5Kcu0WbX8gyakkb2y2m1JLTKuoRyYZoZ8Crq+q+5KcBxxNcldVHV/fKMmZwLuA/9ZCP6WdcyVE9dzYgF5VJ4GTw9tfS3ICuAg4vqHpLwC3Az/QdCelHXMzZi2BqapckuwBLgOObHj8IuAfAr/RVMekRlmCqCUwcUBPci6DEfh1VfXEhsPvBt5eVU+PeY79SVaTrK6trU3fW2kSO1wJUVpUE80UTXI2cCfwX6vqpk2OfxbI8O6FwDeA/VX1+6Oe05miasVWqZXNcujSgtlqpujYHHqSALcAJzYL5gBV9d3r2r8fuHOrYC61ZocrIUqLbJIql8uBq4FjSe4fPnYDsBugqm5uqW/S9E6nVk6P0E2taIlMUuXyMb6VThmrqn5qJx2SJmYZovQMzhTVYrIMUXoWF+eaQ+43OYFtliF6bdVnjtDnjEtwb2Kz1Mo2cuVeW/WdAX3OuN/kBjvckHk9r636zoA+ZyzS2KDBMkSvrfrOgD5n+lakseO5PA1G4b5dW2kj9xRVa6bKWW8V+Z3hKf2NHc0UlbZr4pz1uMhvGaI0EcsW1ZqJ18NyJUSpEY7Q1ZqJc9Z+Wik1woCuVj0rW+J0fak1BnR1x+n6UqvMoas75sqlVhnQ1Q53DZI6Z8pFzWtwur6kyRnQ1Tx3DZJmwpSLmmdqRZoJR+jaGcsQpblhQNf2WYYozRVTLto+yxCluWJAXzLb3oLNMkRp7plyWSLb3oLNMkRpIRjQl8i2t2CzDFFaCGNTLkkuTnJPkuNJHkpy7SZt3pLkgSTHkvyvJK9op7vaiW1nSEytSAthkhH6KeD6qrovyXnA0SR3VdXxdW0+C/xwVX0lyeuAA8CrWuivdmBshmTUzkCmVqSFMPUWdEn+AHhPVd014vjzgQer6qKtnsct6ObMthPskrq01RZ0U1W5JNkDXAYc2aLZNcBHRnz//iSrSVbX1tameWm1zRJEaeFNHNCTnAvcDlxXVU+MaPMjDAL62zc7XlUHqmqlqlZ27dq1nf6qCZYgSr00UZVLkrMZBPPbquqOEW2+H/ht4HVV9VhzXVSjLEGUemtsQE8S4BbgRFXdNKLNbuAO4Oqq+nSzXVSjLEGUemuSEfrlwNXAsST3Dx+7AdgNUFU3A78EXAD8+iD+c2pU0l4z5obMUm+NDehV9TEgY9r8DPAzTXVKDXElRGmpOFO0r1wJUVo6Ls41Q9teKGsSliFKS8cR+ow0Oo9ns9SKuXJp6RjQZ2TbC2VtZBmipCED+ow0NoC2DFHSkAF9RhobQJtakTRkQJ+hqQfQliFK2oIBfVFYhihpDMsWF4VliJLGMKDPm1HF6a6GKGkMUy4tGbX5z9hv2iqtYq5c0hYWLqBvK1B2bNuThsYVp5srl7SFhQroi7JL2rYnDVmCKGkHFiqHviifC06U7t4sV346rXLjjfP710rS3FqoEfqiDGDHprstQZTUgoUK6Iv0ueCWcbmxhVwk6VsWKqDDAg5gXQlRUkcWLqAvFFdClNQhA3qbXAlRUocWqspl4WxR7tLqbkWSlpIj9KZMsRLiotTTS1osBvQmTFmGaJGLpDaMTbkkuTjJPUmOJ3koybWbtEmSX0vycJIHkryyne7OqSlnPLnOlqQ2TDJCPwVcX1X3JTkPOJrkrqo6vq7N64CXDL9eBfzG8N9+GbWQzJRliBa5SGrD2IBeVSeBk8PbX0tyArgIWB/QrwJuraoC7k1yfpIXDL+3HxpeCdEiF0lNmyqHnmQPcBlwZMOhi4DPrbv/6PCx/gR0V0KUNOcmLltMci5wO3BdVT2xnRdLsj/JapLVtbW17TzF7Jj4ljTnJhqhJzmbQTC/raru2KTJ54GL191/4fCxZ6iqA8ABgJWVlZq6t11xM2ZJC2hsQE8S4BbgRFXdNKLZQeCfJfldBh+GPr6w+XNXQpS0oCYZoV8OXA0cS3L/8LEbgN0AVXUz8EfA64GHgW8AP918VztikbikBTVJlcvHgIxpU8DPN9WpzrgSoqQeWd6Zoq6EKKlnljeguxKipJ5Z3tUWLUOU1DPLMUK3DFHSEuh/QLcMUdKS6H/KZcqVECVpUfUroG+2DZC5cklLoj8pF8sQJS25/gR0yxAlLbn+pFxMrUhacos3Qh+1a5CpFUlLbrEC+lYliGBqRdJSW6yUiyWIkjTSYgV08+SSNNJipVzMk0vSSIsV0ME8uSSNsFgpF0nSSAZ0SeoJA7ok9YQBXZJ6woAuST1hQJeknkhVzeaFkzXg68CXZtKB+XEhy30Nlv38wWsAXoNpzv9FVbVrswMzC+gASVaramVmHZgDy34Nlv38wWsAXoOmzt+UiyT1hAFdknpi1gH9wIxffx4s+zVY9vMHrwF4DRo5/5nm0CVJzZn1CF2S1JBOAnqSK5P8WZKHk/zzTY5/W5IPDo8fSbKni351ZYLz/8Ukx5M8kOTuJC+aRT/bNO4arGv3j5JUkt5VPExyDZL8+PBn4aEk/6nrPrZpgt+D3UnuSfLJ4e/C62fRz7Yk+Q9JvpjkwRHHk+TXhtfngSSvnPpFqqrVL+BM4C+A7wHOAT4FvGxDm38K3Dy8/Sbgg233q6uvCc//R4BvH97+uT6d/6TXYNjuPOCjwL3Ayqz7PYOfg5cAnwSeP7z/nbPud8fnfwD4ueHtlwF/Oet+N3wN/i7wSuDBEcdfD3wECPBq4Mi0r9HFCP0HgYer6jNV9STwu8BVG9pcBXxgePvDwBVJ0kHfujD2/Kvqnqr6xvDuvcALO+5j2yb5GQC4EXgX8P+67FxHJrkG/wR4b1V9BaCqvthxH9s0yfkX8B3D288DvtBh/1pXVR8FvrxFk6uAW2vgXuD8JC+Y5jW6COgXAZ9bd//R4WObtqmqU8DjwAUd9K0Lk5z/etcw+CvdJ2OvwfDt5cVV9YdddqxDk/wcvBR4aZKPJ7k3yZWd9a59k5z/LwNvTfIo8EfAL3TTtbkxbax4lsXbsajHkrwVWAF+eNZ96VKSM4CbgJ+acVdm7SwGaZd9DN6lfTTJJVX11Zn2qjs/Aby/qv5tkr3A7yR5eVU9PeuOLYouRuifBy5ed/+Fw8c2bZPkLAZvtx7roG9dmOT8SfJa4B3AG6rqmx31rSvjrsF5wMuBQ0n+kkH+8GDPPhid5OfgUeBgVf11VX0W+DSDAN8Hk5z/NcCHAKrqMPAcBmucLIuJYsVWugjo/xt4SZLvTnIOgw89D25ocxD4yeHtNwJ/UsNPCXpg7PknuQz4TQbBvE9509O2vAZV9XhVXVhVe6pqD4PPEd5QVauz6W4rJvk9+H0Go3OSXMggBfOZLjvZoknO/xHgCoAk38cgoK912svZOgi8bVjt8mrg8ao6OdUzdPTp7usZjDb+AnjH8LF3MvilhcF/3O8BDwOfAL5n1p9Id3z+/x34v8D9w6+Ds+5z19dgQ9tD9KzKZcKfgzBIPR0HjgFvmnWfOz7/lwEfZ1ABcz/wY7Puc8Pn/5+Bk8BfM3g3dg3ws8DPrvv/f+/w+hzbzu+AM0UlqSecKSpJPWFAl6SeMKBLUk8Y0CWpJwzoktQTBnRJ6gkDuiT1hAFdknri/wPuHNGr33k4DQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUiBpfzz7MLu",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Custom training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AXQwWxJ7MLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29iRg5l7MLx",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36CPUq827MLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYAKcqFV7ML1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the custom layers and model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTleeosW7ML4",
        "colab_type": "text"
      },
      "source": [
        "#### Load the reuters dataset and define the class_names "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7tBL9T_7ML5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
        "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
        "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
        "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
        "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLn-j5zn7ML7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the class of the first sample\n",
        "\n",
        "print(\"Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTEaudvN7ML8",
        "colab_type": "text"
      },
      "source": [
        "#### Get the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1LPw8PY7ML9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Reuters word index\n",
        "\n",
        "word_to_index = reuters.get_word_index()\n",
        "\n",
        "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
        "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8fPH-V97ML_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first data example sentence\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ryLItfi7MMB",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Z5urhf7MMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function that encodes the data into a 'bag of words' representation\n",
        "\n",
        "def bag_of_words(text_samples, elements=10000):\n",
        "    output = np.zeros((len(text_samples), elements))\n",
        "    for i, word in enumerate(text_samples):\n",
        "        output[i, word] = 1.\n",
        "    return output\n",
        "\n",
        "x_train = bag_of_words(train_data)\n",
        "x_test = bag_of_words(test_data)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myUlJq2_7MME",
        "colab_type": "text"
      },
      "source": [
        "#### Define the loss function and optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW621La67MMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the categorical cross entropy loss and Adam optimizer\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def loss(model, x, y, wd):\n",
        "    kernel_variables = []\n",
        "    for l in model.layers:\n",
        "        for w in l.weights:\n",
        "            if 'kernel' in w.name:\n",
        "                kernel_variables.append(w)\n",
        "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
        "    y_ = model(x)\n",
        "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuk_pQ7A7MMI",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiIpRQDI7MMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to compute the forward and backward pass\n",
        "\n",
        "def grad(model, inputs, targets, wd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, wd)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCL_MqMc7MML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement the training loop\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "    \n",
        "    \n",
        "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V7P3raP7MMN",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9qkYN8Q7MMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Dataset object for the test set\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPTrgjWp7MMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect average loss and accuracy\n",
        "\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oZqFqL17MMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop over the test set and print scores\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    # Optimize the model\n",
        "    loss_value = loss(model, x, y, weight_decay)    \n",
        "    # Compute current loss\n",
        "    epoch_loss_avg(loss_value)  \n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(to_categorical(y), model(x))\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
        "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWtoHyZK7MMT",
        "colab_type": "text"
      },
      "source": [
        "#### Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDMXn_Ae7MMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the training loss and accuracy\n",
        "\n",
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeCqC697MMV",
        "colab_type": "text"
      },
      "source": [
        "#### Predict from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZFvutuD7MMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the model prediction for an example input\n",
        "\n",
        "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
        "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
        "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GQvkxOn7MMX",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyBnRa5J7MMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQWvJS8y7MMZ",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOkzC3H37MMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a new model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laUe5LLl7MMe",
        "colab_type": "text"
      },
      "source": [
        "#### Redefine the grad function using the @tf.function decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq21Pxot7MMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the @tf.function decorator\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8IOTSRz7MMn",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDo3PG2Z7MMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-run the training loop\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01XZhNmj7MMt",
        "colab_type": "text"
      },
      "source": [
        "#### Print the autograph code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaguRKMp7MMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use tf.autograph.to_code to see the generated code\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}